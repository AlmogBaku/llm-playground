![/LICENSE](https://img.shields.io/github/license/AlmogBaku/llm-playground.svg)
![/issues](https://img.shields.io/github/issues/AlmogBaku/llm-playground.svg)
![/stargazers](https://img.shields.io/github/stars/AlmogBaku/llm-playground.svg)

<h1><picture>
  <source media="(prefers-color-scheme: dark)" srcset=".github/logo-light.svg">
  <img alt="LLM Playground" src="./src/assets/logo.svg">
</picture></h1>

LLM Playground is a versatile environment for experimenting with different large language models (LLMs). It facilitates basic evaluation and comparisons directly in your browser, without the need to set up projects or write code in Jupyter notebooks. This tool supports a variety of LLMs, including OpenAI models, through configurable endpoints.

<picture>
  <source media="(prefers-color-scheme: dark)" srcset="./.github/screencast-dark.gif">
  <img alt="LLM Playground screenshot" src=".github/screencast.gif">
</picture>

## Features

- **Flexible Configuration**: Use environmental variables, a settings YAML file, or a `.env` file.
- **Support for Multiple Vendors**: Compatible with OpenAI protocol (you can configure other providers by setting up an [LiteLLM Proxy](https://docs.litellm.ai/docs/simple_proxy)).
- **Easy to Use**: Designed for straightforward setup and minimal overhead.
- **Rate experiments**: Compare different experiments by categorizing them and rating their performance.

## Getting Started

### Prerequisites

- Docker installed on your machine.

### Installation

To get started with LLM Playground, you can use Docker to pull and run the container:

```bash
docker pull ghcr.io/almogbaku/llm-playground
docker run -p 8080:8080 ghcr.io/almogbaku/llm-playground
```

This will start the LLM Playground on port 8080.

## Configuration

LLM Playground allows various configuration methods including environment variables, a `.env` file, or a `settings.yml` file.

### Configuration Options

- `openai_api_key`: Your OpenAI API key.
- `openai_organization`: Your OpenAI organization ID.
- `openai_base_url`: Base URL for the OpenAI API.
- `models`: Configuration for the models and endpoints.
  - `urls`: An array of URLs to fetch models from an LLM-Playground compatible API.
  - `oai_urls`: An array of URLs to fetch models from an OpenAI compatible API.
  - `models`: An array of Model objects. Each [Model object](server/src/protocol.py#L8) can have the following properties:
    - `name`: The name of the model.
    - `description`: A description of the model.
    - `type`: The type of the model, can be either 'chat' or 'completions'.
    - `system_prompt`: A boolean value indicating whether the model supports a System Prompts.
    - `max_tokens`: The maximum number of tokens that can be generated by the model.
    - `vendor`: The vendor of the model.
    - `api_key`: The API key for the model. (Optional; otherwise, the `openai_api_key` is used)
    - `base_url`: The base URL for the model's API. (Optional; otherwise, the `openai_base_url` is used)

### Models Configuration

Configure your models using one of the following methods:

1. **Direct Configuration**(`modeld.models`): Directly specifying the models in the configuration.
2. **API Provider URLs**(`models.urls`): Fetching the models from an LLM Playground compatible API ([GET request that returns an array of Model](server/src/protocol.py#L8)).
3. **OpenAI API URLs**(`models.oai_urls`): Fetching the models from an OpenAI compatible API.

Each model can be configured with a `base_url` if it does not utilize OpenAI or is not fetched from `models.oai_urls`.

### Example Configuration

Here is a more detailed example using an environment variable setup:

```bash
export OPENAI_API_KEY="your-openai-api-key"

export MODELS_MODELS_0_NAME="LLama3"
export MODELS_MODELS_0_DESCRIPTION="Facebook's Llama3 Model"
export MODELS_MODELS_0_TYPE="chat"
export MODELS_MODELS_0_MAX_TOKENS="32000"
export MODELS_MODELS_0_VENDOR="Facebook"
export MODELS_MODELS_0_BASE_URL="https://api.example.com/v1/chat/completions
```

For multiple models, repeat the pattern adjusting the `MODEL_#_` prefix.

### YAML Configuration Example

```yaml
openai_api_key: "your-openai-api-key"
models:
  urls: ["https://api.example.com/api/models"] # Fetch models from an LLM-Playground compatible API
  oai_urls: ["https://api.openai.com/api/v1/models"] # Fetch models from an OpenAI compatible API
  models:
    - name: "llama3"
      description: "Facebook's Llama3 Model"
      type: "chat"
      base_url: "https://api.example.com/v1/chat/completions"
      max_tokens: 32000
      vendor: "Facebook"
```

## Usage

Once deployed, access LLM Playground by visiting `http://localhost:8080`. Choose from the available models to start your experiments and comparisons.
